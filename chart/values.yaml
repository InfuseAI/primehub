primehub:
  mode: ce
  scheme: http
  keycloak:
    ## Only used when 'keycloak.deploy' is false
    scheme: http
    domain:
    #port:
    username: keycloak
    password:
    #svcUrl: http://keycloak-http.default/auth

    realm: primehub
    everyoneGroupId:
    clientId: admin-ui
    #rolePrefix:
    #maxSockets:
    #maxFreeSockets:
  sharedVolumeStorageClass: ""
  grantSudo: true

ingress:
  enabled: true
  annotations:
    ingress.kubernetes.io/affinity: cookie
    kubernetes.io/ingress.class: nginx
    kubernetes.io/tls-acme: "true"
  hosts:
    - chart-example.local
  tls: []

# Keycloak
keycloak:
  deploy: true
  username: keycloak
  password: ""
  database:
    vendor: postgres
    hostname: keycloak-postgres
    database: keycloak
    port: 5432
    username: keycloak
    password: ""
  dbchecker:
    enabled: true
    image:
      repository: library/busybox
      tag: 1.32
      pullPolicy: IfNotPresent
    securityContext:
      allowPrivilegeEscalation: false
      runAsUser: 1000
      runAsGroup: 1000
      runAsNonRoot: true
    resources:
      requests:
        cpu: 20m
        memory: 32Mi
      limits:
        cpu: 20m
        memory: 32Mi
  securityContext:
    runAsUser: 1000
    runAsNonRoot: true
  service:
    type: ClusterIP
    httpPort: 80
    httpsPort: 8443
  image:
    repository: quay.io/keycloak/keycloak
    tag: "19.0.3"
    pullPolicy: IfNotPresent
  replicas: 1
  metrics:
    enabled: true
  health:
    enabled: true
  # Liveness probe configuration
  livenessProbe: |
    httpGet:
      path: /auth/health/live
      port: http
    initialDelaySeconds: 0
    timeoutSeconds: 5
  # Readiness probe configuration
  readinessProbe: |
    httpGet:
      path: /auth/health/ready
      port: http
    initialDelaySeconds: 10
    timeoutSeconds: 1
  # Startup probe configuration
  startupProbe: |
    httpGet:
      path: /auth/health
      port: http
    initialDelaySeconds: 15
    timeoutSeconds: 1
    failureThreshold: 60
    periodSeconds: 5
  resources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 50m
      memory: 64Mi
  podManagementPolicy: OrderedReady
  extraVolumeMounts: ""
  extraContainers: ""
  podSecurityContext:
    fsGroup: 1000
  enableServiceLinks: true
  restartPolicy: Always
  nodeSelector: {}
  tolerations: []
  terminationGracePeriodSeconds: 60
  theme:
    image: infuseai/primehub-keycloak-theme:846cbfc3
  extraInitContainers: |
    - name: theme-provider
      image: {{ .Values.keycloak.theme.image }}
      imagePullPolicy: IfNotPresent
      command:
        - sh
      args:
        - -c
        - |
          echo "Copying theme..."
          cp -R /primehub/* /theme-primehub/
      volumeMounts:
        - name: theme-primehub
          mountPath: /theme-primehub
  extraVolumeMounts: |
    - name: theme-primehub
      mountPath: /opt/keycloak/themes/primehub
  extraVolumes: |
    - name: theme-primehub
      emptyDir: {}
  eula:
    enabled: false

# Keycloak Database - PostgreSQL
postgresql:
  auth:
    database: keycloak
    username: keycloak
    password: ""
  service:
    port: 5432
  containerPort: 5432
  updateStrategy:
    type: RollingUpdate
    rollingUpdate: {}
  nodeSelector: {}
  tolerations: []
  terminationGracePeriodSeconds: ""
  containerSecurityContext:
    enabled: true
    runAsUser: 1001
  podSecurityContext:
    enabled: true
    fsGroup: 1001
  initContainers: []
  image:
    repository: bitnami/postgresql
    tag: 14.5.0-debian-11-r35
  persistence:
    enabled: true
    mountPath: /bitnami/postgresql
    dataDir: /bitnami/postgresql/data
    size: 8Gi
    #storageClassName: ""
  resources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 50m
      memory: 64Mi
  livenessProbe:
    enabled: true
    initialDelaySeconds: 30
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1
  shmVolume:
    enabled: true
    sizeLimit: ""

# Metacontroller
metacontroller:
  deploy: true
  replicas: 1
  image:
    repository: metacontroller/metacontroller
    tag: 0.2
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 50m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 32Mi
  nodeSelector: {}
  tolerations: []
  affinity: {}
  webhook:
    replicas: 1
    image:
      repository: metacontroller/jsonnetd
      tag: 0.1
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 50m
        memory: 128Mi
      requests:
        cpu: 50m
        memory: 128Mi
    nodeSelector: {}
    tolerations: []
    affinity: {}

# Primehub console
console:
  enableUserPortal: true
  enableInviteUsers: false
  npsSurvey:
    enabled: true
  replicas: 1
  reloaderImage:
    repository: library/busybox
    pullPolicy: IfNotPresent
    tag: 1.32
  image:
    repository: infuseai/primehub-console
    tag: v4.0-b0fe5346
    # pullPolicy: IfNotPresent
    # credentials:
    #   registry: registry.gitlab.com
    #   username:
    #   password:

  url: https://example.com/
  graphqlEndpoint: /graphql
  #locale: en

  service:
    type: ClusterIP
    port: 80
    targetPort: 3000
  resources:
    limits:
      cpu: 100m
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

graphql:
  replicas: 1
  reloaderImage:
    repository: library/busybox
    pullPolicy: IfNotPresent
    tag: 1.32
  zipper:
    repository: infuseai/primehub-sharedfiles-zipper
    tag: v1.0.0
    targetPort: 4000
  image:
    repository: infuseai/primehub-console-graphql
    tag: v4.0-b0fe5346
    # pullPolicy: IfNotPresent
    # credentials:
    #   registry: registry.gitlab.com
    #   username:
    #   password:
  sharedGraphqlSecret:
  #playgroundEnabled: false

  service:
    type: ClusterIP
    port: 80
    targetPort: 3001
  resources:
    limits:
      cpu: 1000m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 512Mi
  probe: {}
  #  timeoutSeconds: 1

  nodeSelector: {}

  tolerations: []

  affinity: {}

watcher:
  image:
    repository: infuseai/primehub-console-watcher
    tag: v4.0-b0fe5346
    # pullPolicy: IfNotPresent
    # credentials:
    #   registry: registry.gitlab.com
    #   username:
    #   password:
  resources:
    limits:
      cpu: 50m
      memory: 256Mi
    requests:
      cpu: 50m
      memory: 128Mi

# Admisson
admission:
  image:
    repository: infuseai/primehub-admission
    tag: db8a8ba960
    pullPolicy: IfNotPresent
  podImageReplacing:
    imagePrefix: "primehub.airgap:5000/"
  resources:
    limits:
      cpu: 1000m
      memory: 256Mi
    requests:
      cpu: 128m
      memory: 128Mi
  postHook:
    resources:
      limits:
        cpu: 200m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi

# Bootstrap
bootstrap:
  enabled: true
  image:
    repository: infuseai/primehub-bootstrap
    tag: "20221125"
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 500m
      memory: 128Mi
    requests:
      cpu: 50m
      memory: 128Mi
  username: phadmin
  password:
  email:
  enforceUpdatePassword: false
  group: phusers
  instanceTypes:
  - metadata:
      name: cpu-1
    spec:
      description: 1 CPU / 2G
      displayName: CPU 1
      limits.cpu: 1
      limits.memory: 2G
      limits.nvidia.com/gpu: 0
      requests.cpu: 1
      requests.memory: 2G
      tolerations:
      - key: "hub.jupyter.org/dedicated"
        operator: "Equal"
        value: "user"
        effect: "NoSchedule"
  - metadata:
      name: cpu-2
    spec:
      description: 2 CPU / 10G
      displayName: CPU 2
      limits.cpu: 2
      limits.memory: 10G
      limits.nvidia.com/gpu: 0
      nodeSelector: {}
      tolerations:
      - key: "hub.jupyter.org/dedicated"
        operator: "Equal"
        value: "user"
        effect: "NoSchedule"
  - metadata:
      name: gpu-1
    spec:
      description: 2 CPU / 7G / 1 GPU
      displayName: GPU 1
      limits.cpu: 2
      limits.memory: 7G
      limits.nvidia.com/gpu: 1
      requests.cpu: 1.5
      requests.memory: 5G
      tolerations:
      - key: "hub.jupyter.org/dedicated"
        operator: "Equal"
        value: "user"
        effect: "NoSchedule"
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule
  - metadata:
      name: gpu-2
    spec:
      description: 4 CPU / 14G / 2 GPU
      displayName: GPU 2
      limits.cpu: 4
      limits.memory: 14G
      limits.nvidia.com/gpu: 2
      requests.cpu: 4
      requests.memory: 14G
      tolerations:
      - key: "hub.jupyter.org/dedicated"
        operator: "Equal"
        value: "user"
        effect: "NoSchedule"
      - key: nvidia.com/gpu
        operator: Exists
        effect: NoSchedule


# Dataset upload
datasetUpload:
  enabled: true
  interface:
    replicas: 1
    webFrontEndImage:
      repository: infuseai/dataset-upload-web-front-end
      tag: d744507a5e
      pullPolicy: IfNotPresent
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 200m
          memory: 256Mi
    tusd:
      resources:
        limits:
          cpu: 200m
          memory: 128Mi
        requests:
          cpu: 200m
          memory: 128Mi

tusd:
  replicas: 1
  image:
    repository: tusproject/tusd
    tag: v1.10.0
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 200m
      memory: 128Mi
    requests:
      cpu: 200m
      memory: 128Mi

# Jupyterhub
jupyterhub:
  enabled: true

  hub:
    # Maximum number of consecutive failures to allow before shutting down JupyterHub.
    consecutiveFailureLimit: 0
    extraEnv:
      KC_CLIENT_SECRET:
        valueFrom:
          secretKeyRef:
            name: primehub-client-jupyterhub
            key: client_secret
      GRAPHQL_SHARED_SECRET:
        valueFrom:
          secretKeyRef:
            name: primehub-graphql-shared-secret
            key: sharedSecret
    extraContainers: []
    extraVolumes:
    - name: primehub-hub-images
      configMap:
        name: primehub-hub-images
    - name: primehub-hub-js
      configMap:
        name: primehub-hub-js
    - name: primehub-hub-css
      configMap:
        name: primehub-hub-css
    - name: primehub-hub-templates
      configMap:
        name: primehub-hub-templates
    - name: primehub-hub-config
      configMap:
        name: primehub-hub-config
    extraVolumeMounts:
    - name: primehub-hub-images
      mountPath: /usr/local/share/jupyterhub/static/images
      readOnly: true
    - name: primehub-hub-js
      mountPath: /usr/local/share/jupyterhub/static/components/primehub
      readOnly: true
    - name: primehub-hub-css
      mountPath: /usr/local/share/jupyterhub/static/css/primehub
      readOnly: true
    - name: primehub-hub-templates
      mountPath: /etc/jupyterhub/templates
      readOnly: true
    - name: primehub-hub-config
      mountPath: /srv/primehub
      readOnly: true
    image:
      name: jupyterhub/k8s-hub
      tag: '2.0.0'
    networkPolicy:
      enabled: true
    extraConfig:
      primehub_config_main.py: |
        exec(open("/srv/primehub/primehub_config.py").read())
    resources:
      limits:
        cpu: 1000m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 512Mi
    containerSecurityContext:
      runAsUser: 1000
      runAsGroup: 1000
      allowPrivilegeEscalation: true

  proxy:
    https:
      enabled: false
    service:
      type: ClusterIP
      # this is required for switching from the default LoadBalancer install
      nodePorts:
        http: "null"
        https: "null"
    chp:
      resources:
        limits:
          cpu: 200m
          memory: 512Mi
        requests:
          cpu: 200m
          memory: 512Mi

    config:
      JupyterHub:
        admin_access: true
        authenticator_class: OIDCAuthenticator

  singleuser:
    uid: 0
    fsGid: null
    cloudMetadata:
      # block set to true will append a privileged initContainer using the
      # iptables to block the sensitive metadata server at the provided ip.
      blockWithIptables: true
      ip: 169.254.169.254
    events: true
    networkPolicy:
      enabled: true
    extraEnv:
      JUPYTER_ENABLE_LAB: "yes"
      CULL_TIMEOUT: "7200"
      CULL_KERNEL_TIMEOUT: "3600"
      CULL_INTERVAL: "300"
      CULL_CONNECTED: "1"
    nodeSelector:
      component: singleuser-server
    image:
      name: infuseai/docker-stacks
      tag: base-notebook-1eb85fe1
      pullPolicy: IfNotPresent
    cmd: null
    networkTools:
      image:
        name: jupyterhub/k8s-network-tools
        tag: '2.0.0'
        pullPolicy: ''
        pullSecrets: []
    initContainers: []
    storage:
      extraVolumes:
      - name: primehub-examples
        emptyDir: {}
      - name: share-memory-device
        emptyDir:
          medium: Memory
      extraVolumeMounts:
      - name: primehub-examples
        mountPath: /primehub-examples
      - name: share-memory-device
        mountPath: /dev/shm

  # prePuller relates to the hook|continuous-image-puller DaemonsSets
  prePuller:
    annotations: {}
    resources:
      limits:
        cpu: 50m
        memory: 256Mi
      requests:
        cpu: 50m
        memory: 256Mi
    extraTolerations: []

    # hook relates to the hook-image-awaiter Job and hook-image-puller DaemonSet
    hook:
      enabled: false
      nodeSelector: {}
      tolerations: []
      resources:
        limits:
          cpu: 50m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 256Mi
    continuous:
      enabled: false

  scheduling:
    userScheduler:
      enabled: false
      resources:
        limits:
          cpu: 50m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 256Mi

  ingress:
    enabled: false

  debug:
    enabled: false

  cull:
    enabled: false

  primehub:
    keycloakClientId: jupyterhub
    scopeRequired: ''
    startnotebook: {}
    kernelGateway: false
    node-affinity-preferred: []
    node-affinity-required: []
    pod-affinity-preferred: []
    pod-affinity-required: []
    pod-anti-affinity-preferred: []
    pod-anti-affinity-required: []
    startNotebookConfigMap: start-notebook-d

    # timeout from spawning to spawned
    spawnerStartTimeout: 1200
    # timeout for the http server in a spawned pod (the creating phase in the progress)
    spawnerHttpTimeout: 30

    # primehub-examples image
    primehub-examples:
      repository: infuseai/primehub-examples
      pullPolicy: IfNotPresent
      tag: v0.1.1

# Primehub controller
controller:
  replicaCount: 1

  airgap:
    imagePrefix: ""

  image:
    repository: infuseai/primehub-controller
    tag: v4.0-6c5467e2
    # pullPolicy: IfNotPresent

  proxy:
    image:
      repository: gcr.io/kubebuilder/kube-rbac-proxy
      tag: v0.4.1

  service:
    type: ClusterIP
    port: 8443

  resources:
    limits:
      cpu: 100m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 20Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

# Image builder
customImage:
  ## Configure the image registry and repository to push.
  ## For more information, please see https://docs.primehub.io/docs/next/getting_started/configure-image-builder

  #registryEndpoint: https://gcr.io
  #registryUsername: _json_key
  #registryPassword: _password_
  #pushRepo: gcr.io/<project>/custom-image
  #pushRepoPrefix: gcr.io/<project>
  insecureSkipVerify: false
  pushSecretName: primehub-controller-custom-image-push-secret

  buildJob:
    image:
      repository: quay.io/buildah/stable
      tag: v1.28.0
    resources:
      limits:
        cpu: 2000m
        memory: 1500Mi
      requests:
        cpu: 500m
        memory: 100Mi

# Group Volume (shared volume over nfs)
groupvolume:
  enabled: true
  storageClass: null
  nfs:
    image:
      repository: registry.k8s.io/volume-nfs
      tag: 0.8
      pullPolicy: IfNotPresent

# Gitsync Dataset
gitsync:
  enabled: true
  daemonset:
    delayInit: false
    image:
      repository: registry.k8s.io/git-sync
      tag: v3.1.3
      pullPolicy: IfNotPresent

# Telemetry
telemetry:
  enabled: true

# PrimeHub App
app:
  enabled: true

#                   ____________   ______           __
#                  / ____/ ____/  / ____/__  ____ _/ /___  __________  _____
#  ____________   / __/ / __/    / /_  / _ \/ __ `/ __/ / / / ___/ _ \/ ___/  ____________
# /_____/_____/  / /___/ /___   / __/ /  __/ /_/ / /_/ /_/ / /  /  __(__  )  /_____/_____/
#               /_____/_____/  /_/    \___/\__,_/\__/\__,_/_/   \___/____/


# Job Submission
jobSubmission:
  enabled: true
  workingDirSize: 5Gi
  defaultActiveDeadlineSeconds: 86400
  defaultTTLSecondsAfterFinished: 604800
  jobTTLSeconds: 2592000
  jobLimit: 4000
  # Artifact. Require phfs
  artifact:
    enabled: true
    limitSizeMb: 100
    limitFiles: 1000
    # how long will the artifact would be removed.
    retentionSeconds: 604800
  monitoring:
    enabled: true
  smtp:
    enabled: false
    # host: smtp-domain.example.com
    # port: 587
    # from: noreply@example.com
    # fromDisplayName: PrimeHub Notification
    # username: your-username
    # password: your-password

monitoringAgent:
  image:
    repository: infuseai/primehub-monitoring-agent
    tag: v0.1.2
    pullPolicy: IfNotPresent

# Model Deployment
modelDeployment:
  enabled: true
  mlflowVersion: 2
  engineContainer:
    image:
      repository: seldonio/seldon-core-executor
      tag: 1.4.0
      pullPolicy: IfNotPresent
  modelStorageInitializer:
    image:
      repository: kfserving/storage-initializer
      tag: v0.4.0
      pullPolicy: IfNotPresent
  mlflow1ModelStorageInitializer:
    image:
      repository: infuseai/mlflow-model-downloader
      tag: v0.1.1
      pullPolicy: IfNotPresent
  mlflow2ModelStorageInitializer:
    image:
      repository: infuseai/mlflow-model-downloader
      tag: v0.2.0
      pullPolicy: IfNotPresent

# Admin Notebook (Deprecated)
adminNotebook:
  enabled: false
  replicaCount: 1
  image:
    repository: infuseai/primehub-admin-notebook
    tag: dd029e8112
    pullPolicy: IfNotPresent

  service:
    type: ClusterIP
    port: 80

  resources:
    limits:
      cpu: 1
      memory: 128Mi
    requests:
      cpu: 100m
      memory: 128Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

# Grafana
grafana:
  enabled: false

# Keycloak gateway for admin notebook
keycloakGateway:
  image:
    repository: keycloak/keycloak-gatekeeper
    tag: 6.0.1
    pullPolicy: IfNotPresent

openshift:
  scc: false

istio:
  enabled: false
  authService:
    image:
      repository: infuseai/oidc-authservice
      tag: 1dc296d
      pullPolicy: IfNotPresent
    replicaCount: 1
    service:
      type: ClusterIP
      port: 8080

store:
  enabled: true
  accessKey: "AKIAIOSFODNN7EXAMPLE"
  secretKey: "wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY"
  bucket: "primehub"
  createBucket:
    enabled: true
    policy: none
    purge: false

  phfs:
    enabled: true
  logPersistence:
    enabled: true

fluentd:
  enabled: false

  # Buffer configuration: https://docs.fluentd.org/configuration/buffer-section
  flushAtShutdown: false
  flushInterval: "3600s"
  chunkLimitSize: "256m"
  # S3 Configuratio
  storeAs: "txt"
  image:
    repository: fluent/fluentd-kubernetes-daemonset
    tag: v1.11-debian-s3-1
    pullPolicy: IfNotPresent
  resources:
    limits:
      cpu: 512m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 512Mi

  nodeSelector: {}

  tolerations: []

  affinity: {}

minio:
  ## Set default image, imageTag, and imagePullPolicy. mode is used to indicate the
  ##
  image:
    repository: minio/minio
    tag: RELEASE.2022-04-16T04-26-02Z
    pullPolicy: IfNotPresent

  ## Set default image, imageTag, and imagePullPolicy for the `mc` (the minio
  ## client used to create a default bucket).
  ##
  mcImage:
    repository: minio/mc
    tag: RELEASE.2022-11-07T23-47-39Z
    pullPolicy: IfNotPresent

  ## Additional arguments to pass to minio binary
  extraArgs: []

  ## Enable persistence using Persistent Volume Claims
  ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
  ##
  persistence:
    enabled: true
    storageClass: null
    accessMode: ReadWriteOnce
    size: 10Gi

  service:
    type: ClusterIP
    port: 9000

  imagePullSecrets: []

  ingress:
    enabled: false

    # annotation for nginx.ingress.kubernetes.io/proxy-body-size: "8192m"
    maxBodySize: "8192m"

  nodeSelector: {}
  tolerations: []
  affinity: {}

  resources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 150m
      memory: 64Mi

  s3gateway:
    enabled: false
    replicas: 1
    serviceEndpoint: ""
    accessKey: ""
    secretKey: ""

  ## Use minio as GCS (Google Cloud Storage) gateway, you should disable data persistence so no volume claim are created.
  ## https://docs.minio.io/docs/minio-gateway-for-gcs
  gcsgateway:
    enabled: false
    # Number of parallel instances
    replicas: 1
    # credential json file of service account key
    gcsKeyJson: ""
    # Google cloud project-id
    projectId: ""

rclone:
  # use /var/snap/microk8s/common/var/lib/kubelet with microk8s
  kubeletPath: /var/lib/kubelet

  nodeDriverRegistrar:
    image:
      repository: registry.k8s.io/sig-storage/csi-node-driver-registrar
      tag: v2.4.0
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 256Mi
      requests:
        cpu: 50m
        memory: 16Mi

  rclone:
    image:
      repository: infuseai/csi-rclone
      tag: v1.2.0-16-g998c499
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 256Mi
      requests:
        cpu: 50m
        memory: 16Mi
    tolerations: []

  csiAttacher:
    image:
      repository: registry.k8s.io/sig-storage/csi-attacher
      tag: v3.4.0
      pullPolicy: IfNotPresent
    resources:
      limits:
        cpu: 100m
        memory: 256Mi
      requests:
        cpu: 50m
        memory: 16Mi

sshBastionServer:
  enabled: true
  customHostname: ""
  image:
    repository: infuseai/ssh-proxy
    tag: 0.1.11-patch-1
    pullPolicy: IfNotPresent
  replicaCount: 1
  service:
    type: ClusterIP
    port: 2222
    targetPort: 22
  ssh:
    target:
      matchLabels:
        app: jupyterhub
        component: singleuser-server
        ssh-bastion-server/notebook: "true"
      publicKeyApiPort: 8080
  resources:
    limits:
      cpu: 200m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 256Mi
  netpol:
    enabled: true
    kubeApiPort: 6443
usage:
  enabled: true
  dbUser: postgres
  dbPassword: mysecretpassword

  initImage:
    repository: library/busybox
    pullPolicy: IfNotPresent
    tag: 1.32

  image:
    repository: infuseai/primehub-usage
    pullPolicy: IfNotPresent
    tag: v1.0.1

  podSecurityContext:
    enabled: true
    fsGroup: 1001

  appResources:
    limits:
      cpu: 250m
      memory: 256Mi
    requests:
      cpu: 50m
      memory: 256Mi

  monitorResources:
    limits:
      cpu: 150m
      memory: 256Mi
    requests:
      cpu: 50m
      memory: 128Mi

  dbImage:
    repository: bitnami/postgresql
    tag: 14.5.0-debian-11-r35
    pullPolicy: IfNotPresent

  dbResources:
    limits:
      cpu: 500m
      memory: 256Mi
    requests:
      cpu: 50m
      memory: 256Mi

  dbStorage: 64Gi
  dbStorageClass: null

  # cronjob schedule for the report generator
  schedule: "0 0 * * *"

